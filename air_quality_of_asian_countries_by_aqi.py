# -*- coding: utf-8 -*-
"""Air Quality of Asian Countries by AQI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dkJvQrVMNNwcQpGgc4eYkgJgGe3dKCkW

##**IMPORT ALL THE LIBRARIES**

A library is a collection of functions that can be added to Python code and called as necessary, just like any other function. There is no reason to rewrite code that will perform a standard task. With libraries, we can import pre-existing functions and efficiently expand the functionality of your code.

**Pandas**: A powerful library for data manipulation and analysis, providing data structures like DataFrames.


**NumPy:** A library for numerical computations, supporting large, multi-dimensional arrays and matrices.


**Matplotlib:** A plotting library for creating static, interactive, and animated visualizations in Python.


**Seaborn:** A statistical data visualization library built on Matplotlib, offering aesthetically pleasing and informative charts.


**scikit-learn (sklearn):**


**Preprocessing:** Provides tools for scaling, transforming, and normalizing data.


**Model Selection:** Includes utilities like train_test_split for splitting datasets and cross-validation methods.


**RandomForestClassifier:** An ensemble learning method for classification tasks using decision tree models.


**LinearRegression:** A regression algorithm that models relationships between variables.


**Metrics:** Offers functions to evaluate model performance, like accuracy, R², and mean squared error.
"""

#General Basic Libraries
import pandas as pd                 #PANDA
import numpy as np                  #NUMPY
import matplotlib.pyplot as plt     #MATPLOTLIB
import seaborn as sns               #SEABORN

#Feature Scaling Libraries
from sklearn.preprocessing import MinMaxScaler, StandardScaler

#Classification Libaries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

#Regression Libraries
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

#Confusion Matrix Libraries
from sklearn.metrics import confusion_matrix

"""#**LOADING THE DATASET**

**Uploading Files:**

The files.upload() function from the google.colab module opens a file upload dialog, allowing users to upload files directly from their local system into the Colab environment.
Accessing the Uploaded File:

The uploaded files are stored in a dictionary-like structure, where the keys are filenames. The first uploaded filename is accessed using list(uploaded.keys())[0].


Loading the Data:

The pd.read_csv(filename) function reads the uploaded CSV file into a Pandas DataFrame for further analysis and processing.
Exploratory Analysis:

data.head(): Displays the first few rows of the dataset to give a quick overview of the data.
data.info(): Provides a summary of the dataset, including the number of rows, columns, data types, and non-null values.
"""

# Load the data
from google.colab import files
uploaded = files.upload()

filename = list(uploaded.keys())[0]

data = pd.read_csv(filename)

# Display the first few rows and summary information to understand the structure
data.head()
data.info()

"""#**Statistics**

In this code, we focused on analyzing the numeric columns of the dataset by selecting only those with numerical data types using `data.select_dtypes(include=np.number)`.

We then computed a comprehensive statistical summary for these columns, creating a DataFrame that includes key metrics such as the mean, median, standard deviation, variance, minimum, maximum, skewness, and kurtosis. This summary provides a detailed overview of the distribution, variability, and shape of the numeric data, enabling deeper insights into its statistical properties.
"""

#Statistics
numeric_data = data.select_dtypes(include=np.number)

statistics_summary = pd.DataFrame({
    "Mean": numeric_data.mean(),
    "Median": numeric_data.median(),
    "Standard Deviation": numeric_data.std(),
    "Variance": numeric_data.var(),
    "Minimum": numeric_data.min(),
    "Maximum": numeric_data.max(),
    "Skewness": numeric_data.skew(),
    "Kurtosis": numeric_data.kurt()
})

statistics_summary

"""# **Statistics Visualizations**

In this code, we visualized the statistical properties of numeric columns from the dataset through a variety of plots. The `statistics_summary` DataFrame was updated for easier referencing by resetting the index and renaming columns.

Key metrics like mean, median, standard deviation, variance, minimum, maximum, skewness, and kurtosis were visualized using different chart types.

Bar charts were used for the mean and minimum/maximum values, with a horizontal bar chart for the median.

A line chart highlighted standard deviation trends, while a box plot provided an overview of variance.

Skewness was represented with a scatter plot, and kurtosis was visualized with a heatmap.

These plots offered a clear, detailed exploration of the dataset’s numeric features, making statistical patterns easier to interpret.
"""

# Statistics Summary
statistics_summary = statistics_summary.reset_index()
statistics_summary.rename(columns={'index': 'Metric'}, inplace=True)

# 1. Mean: Bar Chart
plt.figure(figsize=(10, 6))
plt.bar(statistics_summary['Metric'], statistics_summary['Mean'])
plt.title('Mean of Numeric Columns')
plt.xlabel('Columns')
plt.ylabel('Mean')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2. Median: Horizontal Bar Chart
plt.figure(figsize=(10, 6))
plt.barh(statistics_summary['Metric'], statistics_summary['Median'], color='orange')
plt.title('Median of Numeric Columns')
plt.xlabel('Median')
plt.ylabel('Columns')
plt.tight_layout()
plt.show()

# 3. Standard Deviation: Line Chart
plt.figure(figsize=(10, 6))
plt.plot(statistics_summary['Metric'], statistics_summary['Standard Deviation'], marker='o')
plt.title('Standard Deviation of Numeric Columns')
plt.xlabel('Columns')
plt.ylabel('Standard Deviation')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 4. Variance: Box Plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=numeric_data, palette='Set2')
plt.title('Box Plot of Numeric Columns (Variance Overview)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 5. Minimum & Maximum: Side-by-Side Bar Charts
fig, ax = plt.subplots(1, 2, figsize=(16, 6))
sns.barplot(x=statistics_summary['Metric'], y=statistics_summary['Minimum'], ax=ax[0], palette='Blues_d')
ax[0].set_title('Minimum Values of Numeric Columns')
ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45)

sns.barplot(x=statistics_summary['Metric'], y=statistics_summary['Maximum'], ax=ax[1], palette='Reds_d')
ax[1].set_title('Maximum Values of Numeric Columns')
ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45)

plt.tight_layout()
plt.show()

# 6. Skewness: Scatter Plot
plt.figure(figsize=(10, 6))
plt.scatter(statistics_summary['Metric'], statistics_summary['Skewness'], color='green')
plt.title('Skewness of Numeric Columns')
plt.xlabel('Columns')
plt.ylabel('Skewness')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 7. Kurtosis: Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(statistics_summary[['Kurtosis']].T, annot=True, cmap='coolwarm', cbar=True)
plt.title('Kurtosis of Numeric Columns')
plt.tight_layout()
plt.show()

"""# **Numeric Data Distribution in Each Stastistics**

In this code, we generated a synthetic dataset with three distributions: normal, uniform, and exponential, using NumPy’s random sampling functions.

We then calculated a detailed statistical summary for each distribution, including metrics like mean, median, standard deviation, variance, minimum, maximum, skewness, and kurtosis.

To visualize these statistics, a series of bar charts were plotted, with each chart focusing on one metric across all distributions.

This approach provides a comprehensive visual representation of the statistical characteristics of the synthetic data, enabling clear comparisons between the different distributions.
"""

# Example numeric_data DataFrame
np.random.seed(0)
numeric_data = pd.DataFrame({
    "normal distribution": np.random.normal(50, 10, 100),
    "uniform distribution": np.random.uniform(30, 70, 100),
    "exponential distribution": np.random.exponential(1, 100)
})

# Calculating statistics summary
statistics_summary = pd.DataFrame({
    "Mean": numeric_data.mean(),
    "Median": numeric_data.median(),
    "Standard Deviation": numeric_data.std(),
    "Variance": numeric_data.var(),
    "Minimum": numeric_data.min(),
    "Maximum": numeric_data.max(),
    "Skewness": numeric_data.skew(),
    "Kurtosis": numeric_data.kurt()
})

# Visualizing the statistics summary
fig, ax = plt.subplots(len(statistics_summary.columns), 1, figsize=(8, 25))

for i, column in enumerate(statistics_summary.columns):
    ax[i].bar(statistics_summary.index, statistics_summary[column], align='center', alpha=0.7)
    ax[i].set_title(column, fontsize=12)
    ax[i].set_ylabel(column, fontsize=10)
    ax[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""#**DATA PRE-PROCESSING**

# **Handling Missing Values**

In this code, we performed a thorough examination and handling of missing values in the dataset.

First, we displayed the first five rows to confirm the dataset was loaded correctly.

Then, we identified columns with missing values and calculated the percentage of missing data for each.

Columns with more than 50% missing values were dropped, while remaining missing values were filled appropriately: numeric columns were filled with their mean, and categorical columns were filled with their mode, if available. Finally, we verified that no missing values remained, ensuring a cleaned dataset ready for further analysis.
"""

# Display the first few rows to confirm the dataset is loaded correctly
print("\nFirst five rows of the dataset:")
print(data.head())

# Check for missing values in the dataset
missing_values = data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values[missing_values > 0])

# Calculate and display the percentage of missing values for each column
missing_percentage = (missing_values / len(data)) * 100
print("\nPercentage of missing values in each column:")
print(missing_percentage[missing_percentage > 0])

# Dropping columns with more than 50% missing data
columns_to_drop = missing_percentage[missing_percentage > 50].index
data.drop(columns=columns_to_drop, inplace=True)
print(f"\nDropped columns with more than 50% missing values: {columns_to_drop.tolist()}")

# Filling remaining missing values
# Fill numeric columns with their mean
data.fillna(data.mean(numeric_only=True), inplace=True)

# Fill categorical columns with their mode, if mode is available
mode_values = data.mode()
if not mode_values.empty:
    data.fillna(mode_values.iloc[0], inplace=True)
    print("\nCategorical missing values handled using mode.")
else:
    print("\nNo mode values available to fill categorical missing data.")

# Check if any columns still have missing values after filling
remaining_missing_values = data.isnull().sum()
print("\nRemaining missing values after handling:")
print(remaining_missing_values[remaining_missing_values > 0])

"""#**Encoding Categorical Data**

In this code, we reviewed and optimized the data types in the dataset for better efficiency and compatibility.

We began by checking the data types of all columns. If a `Date` column was present, it was converted to the `datetime` format using a specific date format (`%d/%m/%Y`).

Additionally, categorical columns, such as `City` and `Country`, were explicitly converted to the `category` data type where applicable. Finally, we displayed the updated data types to confirm the successful conversions, ensuring the dataset was prepared for further processing.
"""

# 1. Check Data Types and Convert if Necessary
print("Data Types Before Conversion:")
print(data.dtypes)

# Convert date column to datetime if it exists
if 'Date' in data.columns:
    data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')

# Convert categorical columns to 'category' dtype if necessary
categorical_columns = ['City', 'Country']  # Add other categorical columns if needed
for col in categorical_columns:
    if col in data.columns:
        data[col] = data[col].astype('category')

print("\nData Types After Conversion:")
print(data.dtypes)

"""#**Handling Duplicates**

In this code, we addressed duplicate entries in the dataset. First, we identified the number of duplicate rows using the duplicated() method and reported the count.

Then, we removed all duplicate rows using the drop_duplicates() method to ensure data integrity and avoid redundancy.

Finally, we confirmed the removal by displaying the new shape of the dataset, indicating the number of remaining rows and columns.
"""

# 2. Handling Duplicates
duplicates = data.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicates}")

# Remove duplicates
data.drop_duplicates(inplace=True)
print(f"Duplicates removed. New data shape: {data.shape}")

"""#**Scaling**

In this code, we normalized the numeric columns in the dataset to bring their values into a standardized range, typically between 0 and 1, using Min-Max Scaling.

First, we identified the numeric columns using their data types.

Then, we applied the `MinMaxScaler` to transform these columns, ensuring all values were scaled proportionally.

To validate the scaling process, we checked and confirmed that the minimum and maximum values of each scaled column were within the desired range.

This step prepares the data for machine learning models that are sensitive to feature magnitudes.
"""

# 3. Normalization or Scaling

# Identify numeric columns for scaling
numeric_cols = data.select_dtypes(include=np.number).columns.tolist()

# Example: Using Min-Max Scaling
scaler = MinMaxScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])
print("\nData after normalization using Min-Max Scaling:")
print(data)

# Validate normalization
for col in numeric_cols:
    print(f"\n'{col}' - Min: {data[col].min()}, Max: {data[col].max()}")

"""#**Feature Engineering**

In this code, we performed an optional feature engineering step by creating a new feature, `Temp_Humidity_Interaction`.

This feature was generated by multiplying the `Temperature` and `Humidity` columns, provided both were present in the dataset.

This interaction term captures the combined effect of these two variables, potentially enhancing the predictive power of machine learning models.

This step illustrates how feature engineering can uncover hidden patterns and improve model performance.
"""

# 4. Feature Engineering
# Example: Creating a new feature, e.g., Temperature-Humidity Interaction
if 'Temperature' in data.columns and 'Humidity' in data.columns:
    data['Temp_Humidity_Interaction'] = data['Temperature'] * data['Humidity']

"""#**Check for Zero or Negative Values**

In this code, we checked specific pollutant-related columns (`PM2.5`, `PM10`, `NO2`, `SO2`, `CO`, `O3`) for the presence of zero or negative values, which are often anomalies or errors in pollution datasets.

For each column that exists in the dataset, we counted and reported the number of negative and zero values.

This step helps identify data quality issues, ensuring that such values are addressed appropriately before further analysis or modeling.
"""

# 5. Check for Zero or Negative Values (Specific to Pollution Data)
pollutant_columns = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3']
for col in pollutant_columns:
    if col in data.columns:
        negative_values = data[col][data[col] < 0].count()
        zero_values = data[col][data[col] == 0].count()
        print(f"{col} - Negative Values: {negative_values}, Zero Values: {zero_values}")

"""#**Visualize Distributions to Check for Skewness, Outliers, or Patterns**

In this code, we visualized the distributions and potential outliers of numeric columns to better understand their characteristics. Using histograms with KDE (Kernel Density Estimation), we examined the distribution of each numeric column for skewness or patterns, which can guide transformations if necessary.

Additionally, we created boxplots to detect outliers by highlighting extreme values visually. Both sets of visualizations were organized into a grid layout for clarity, providing valuable insights into the data’s structure, spread, and potential anomalies. This step ensures a deeper understanding of the numeric data before modeling.
"""

# 6. Visualize Distributions to Check for Skewness, Outliers, or Patterns
plt.figure(figsize=(18, 10))
for i, col in enumerate(numeric_cols):
    plt.subplot(3, 3, i + 1)  # Adjusting the grid size based on the number of numeric columns
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Visualize boxplots for outlier detection
plt.figure(figsize=(18, 10))
for i, col in enumerate(numeric_cols):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

"""# **Data Science Techniques**

**i) Exploratory Data Analysis - EDA**

> Analyze pollutant distributions, correlations, and interactions with environmental factors.

In this code, we visualized the distributions of key pollutant levels (`PM2.5`, `PM10`, `NO2`, `SO2`, `CO`, `O3`) using histograms with KDE (Kernel Density Estimation) overlays for a smooth representation of their density. The visual style was set using Seaborn's `whitegrid` for a clean and professional appearance. Each pollutant’s distribution was plotted in a grid layout (2 rows and 3 columns), allowing for easy comparison across pollutants. This visualization provides insights into the spread, central tendency, and potential skewness of pollutant levels, aiding in further exploratory analysis or preprocessing steps.
"""

# Setting up visual styles
sns.set(style="whitegrid")

# Plot distribution of pollutant levels (PM2.5, PM10, NO2, SO2, CO, O3)
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
pollutants = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3']

for i, pollutant in enumerate(pollutants):
    row, col = divmod(i, 3)
    sns.histplot(data[pollutant], kde=True, ax=axes[row][col])
    axes[row][col].set_title(f'Distribution of {pollutant}')

plt.tight_layout()
plt.show()

"""**Correlation Matrix**

In this code, we calculated the correlation matrix to examine the relationships between pollutants (`PM2.5`, `PM10`, `NO2`, `SO2`, `CO`, `O3`) and environmental factors (`Temperature`, `Humidity`, `Wind Speed`).

 The correlation values, which range from -1 to 1, quantify the strength and direction of the relationships.

 We visualized this matrix using a heatmap, with annotations showing correlation coefficients and a `coolwarm` colormap to highlight positive and negative correlations.

  This visualization provides a clear overview of interdependencies, helping identify strong correlations that may be valuable for further analysis or predictive modeling.
"""

# Calculate correlation matrix for pollutants and environmental factors
correlation_matrix = data[['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'Temperature', 'Humidity', 'Wind Speed']].corr()

# Plot heatmap of correlations
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Pollutants and Environmental Factors")
plt.show()

"""##**Classification**

In this code, we developed a machine learning pipeline to classify air quality based on pollutants and environmental factors:

1. **AQI Calculation**: We computed the Air Quality Index (AQI) using a simple formula involving `PM2.5` and `PM10`.

2. **Categorization**: The AQI was categorized into qualitative labels (`Good`, `Moderate`, `Unhealthy`, `Hazardous`) based on predefined thresholds using a custom function.

3. **Numerical Labeling**: The categories were mapped to numerical labels (`0`, `1`, `2`, `3`) for classification tasks.

4. **Feature and Target Definition**: Features included pollutant levels and environmental factors (`Temperature`, `Humidity`, etc.), while the target was the AQI label.

5. **Data Splitting**: The dataset was divided into training (80%) and testing (20%) sets using `train_test_split`.

6. **Model Training**: A Random Forest Classifier was trained on the training set.

7. **Prediction**: The trained model predicted AQI categories for the test set.

8. **Evaluation**: The model’s performance was assessed using accuracy and a classification report, providing insights into precision, recall, and F1-score for each category.

This pipeline demonstrates the integration of data preprocessing, feature engineering, and machine learning for air quality classification.
"""

# Step 1: AQI calculation
data['AQI'] = (0.5 * data['PM2.5']) + (0.5 * data['PM10'])

# Step 2: Define AQI categories
def categorize_aqi(aqi):
    if aqi <= 50:
        return "Good"
    elif 51 <= aqi <= 100:
        return "Moderate"
    elif 151 <= aqi <= 150:
        return "Unhealthy"
    else:
        return "Hazardous"

data['AQI_Category'] = data['AQI'].apply(categorize_aqi)

# Step 3: Classify AQI categories as numerical labels
category_mapping = {
    "Good": 0,
    "Moderate": 1,
    "Unhealthy": 2,
    "Hazardous": 3
}
data['AQI_Label'] = data['AQI_Category'].map(category_mapping)

# Step 4: Define features and target
features = data[['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'Temperature', 'Humidity', 'Wind Speed']]
target = data['AQI_Label']

# Step 5: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Step 6: Initialize and train the classification model
classifier = RandomForestClassifier(random_state=42)
classifier.fit(X_train, y_train)

# Step 7: Make predictions on the test set
y_pred = classifier.predict(X_test)

# Step 8: Evaluate the model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, zero_division=0))

"""# **Regression**

Linear regression is a statistical technique used in data science to predict the value of a variable based on the value of another variable.

**Defining Features and Target:**

The code selects environmental factors like PM2.5, PM10, NO2, SO2, CO, O3, Temperature, Humidity, and Wind Speed as the independent variables (features).
The target variable (AQI) is the Air Quality Index that we want to predict.


**Data Splitting:**

The data is split into training and testing sets using train_test_split. 80% of the data is used for training, while 20% is held back for testing.


**Model Training:**

A Linear Regression model is created using the LinearRegression() class from scikit-learn. The model is trained on the x_train and y_train data.


**Prediction:**

After training, the model is used to predict AQI values (y_pred) based on the test data (x_test).
"""

# Define features (x) and target (y)
features = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'Temperature', 'Humidity', 'Wind Speed']
target = 'AQI'

x = data[features]  # Independent variables
y = data[target]    # Target variable

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train the linear regression model
model = LinearRegression()
model.fit(x_train, y_train)


# Predict AQI on test data
y_pred = model.predict(x_test)

# Visualize Predicted vs Actual Values
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.7, color="blue", label="Predicted vs Actual")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color="red", linestyle="--", label="Ideal Fit (y=x)")
plt.title("Linear Regression: Predicted vs Actual Values", fontsize=14)
plt.xlabel("Actual AQI (y_test)", fontsize=12)
plt.ylabel("Predicted AQI (y_pred)", fontsize=12)
plt.legend()
plt.grid()
plt.show()

# Residual Plot
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, alpha=0.7, color="purple")
plt.axhline(0, color="red", linestyle="--")
plt.title("Residual Plot", fontsize=14)
plt.xlabel("Predicted AQI (y_pred)", fontsize=12)
plt.ylabel("Residuals (y_test - y_pred)", fontsize=12)
plt.grid()
plt.show()

# Assuming 'y_test' contains the actual values and 'y_pred' contains predicted values
residuals = y_test - y_pred

# Plot the distribution of residuals
plt.figure(figsize=(8, 5))
sns.histplot(residuals, kde=True, bins=30, color='blue', edgecolor='black')
plt.title("Distribution of Residuals")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.axvline(0, color='red', linestyle='--', linewidth=1.5, label='Zero Residual Line')
plt.legend()
plt.tight_layout()
plt.show()

"""#**Model Validation**

**Evaluation Metrics for Regression**
"""

# Print evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

"""**Confusion Matrix**

In this code, we evaluated the classification performance of the model by creating a **confusion matrix** to compare true and predicted AQI categories:

1. **Mapping Predictions and True Values**: The numeric predictions (`y_pred`) and actual test values (`y_test`) were mapped back to their respective AQI categories using the `categorize_aqi` function.

2. **Confusion Matrix**: A confusion matrix was generated to tabulate the count of correct and incorrect predictions across AQI categories (`Good`, `Moderate`, `Unhealthy`, `Hazardous`).

3. **Heatmap Visualization**: The confusion matrix was visualized using a Seaborn heatmap:
   - Each cell represents the count of instances for a given combination of true and predicted categories.
   - Labels for the axes and categories were added for clarity.
   - A color gradient (`Blues`) was applied to highlight the distribution of predictions.

This visualization provides insights into the model's classification performance, indicating where it performs well and where misclassifications occur.
"""

# Map the numeric predictions back to categories
y_test_cat = y_test.apply(categorize_aqi)  # Apply the function to y_test
y_pred_cat = pd.Series(y_pred).apply(categorize_aqi)  # Apply the function to y_pred

# Generate confusion matrix using categorized data
cm = confusion_matrix(y_test_cat, y_pred_cat)

# Plot confusion matrix using heatmap
plt.figure(figsize=(8, 6))  # Adjust size for better readability
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
            xticklabels=["Good", "Moderate", "Unhealthy", "Hazardous"],  # Updated labels
            yticklabels=["Good", "Moderate", "Unhealthy", "Hazardous"])  # Updated labels
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
plt.show()